{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ps8YOkVsYxrd"
   },
   "source": [
    "# 使用RNN网络建模SMILES序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IQDvjCzrYxrf"
   },
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install rdkit\n",
    "# !pip install torch\n",
    "# !pip3 install torch==2.1.0 # 对于 x86 平台 pip3 install torch==2.1.0+cpu --index-url https://download.pytorch.org/whl/cpu\n",
    "# !pip3 install pyyaml \n",
    "# !pip3 install setuptools\n",
    "# !pip3 install torch-npu==2.1.0.post8\n",
    "# !pip install pandas==2.1.2\n",
    "# !pip install lxml==5.1.0\n",
    "# !pip install psutil==5.9.5\n",
    "# !pip install typing-extensions==4.7.1\n",
    "# !pip install urllib3==1.26.18\n",
    "!export PYTHONPATH=usr/local/Ascend/ascend-toolkit/8.0.RC1/aarch64-linux/ascend-toolkit/latest/tools/ms_fmk_transplt/torch_npu_bridge:$PYTHONPATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "iLX0i9GDYxrf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning : ASCEND_HOME_PATH environment variable is not set.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Ascend/ascend-toolkit/latest/tools/ms_fmk_transplt/torch_npu_bridge/transfer_to_npu.py:345: ImportWarning: \n",
      "    *************************************************************************************************************\n",
      "    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..\n",
      "    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..\n",
      "    The backend in torch.distributed.init_process_group set to hccl now..\n",
      "    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..\n",
      "    The device parameters have been replaced with npu in the function below:\n",
      "    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.autocast, torch.load, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.nn.Module.to, torch.nn.Module.to_empty\n",
      "    *************************************************************************************************************\n",
      "    \n",
      "  warnings.warn(msg, ImportWarning)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from typing import List, Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torch_npu\n",
    "import transfer_to_npu\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_rFgzInBYxrf"
   },
   "source": [
    "## 首先，我们定义一个RNN模型\n",
    "可以直接使用pytorch完成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "XlqwmURkYxrg"
   },
   "outputs": [],
   "source": [
    "# 定义RNN模型\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, num_embed, input_size, hidden_size, output_size, num_layers, dropout, device):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.embed = nn.Embedding(num_embed, input_size)\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers=num_layers,\n",
    "                          batch_first=True, dropout=dropout, bidirectional=True)\n",
    "        self.fc = nn.Sequential(nn.Linear(2 * num_layers * hidden_size, output_size),\n",
    "                                nn.Sigmoid(),\n",
    "                                nn.Linear(output_size, 1),\n",
    "                                nn.Sigmoid())\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x : [bs, seq_len]\n",
    "        x = self.embed(x)\n",
    "        # x : [bs, seq_len, input_size]\n",
    "        _, hn = self.rnn(x) # hn : [2*num_layers, bs, h_dim]\n",
    "        hn = hn.transpose(0,1)\n",
    "        z = hn.reshape(hn.shape[0], -1) # z shape: [bs, 2*num_layers*h_dim]\n",
    "        output = self.fc(z).squeeze(-1) # output shape: [bs, 1]\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTY9wRhvYxrg"
   },
   "source": [
    "# 这一部分我们定义数据处理函数，以及tokenizer\n",
    "这里的tokenizer是自定义的，目的是针对SMILES划分token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "w39g-sz1Yxrh"
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "## 数据处理部分\n",
    "# tokenizer，鉴于SMILES的特性，这里需要自己定义tokenizer和vocab\n",
    "# 这里直接将smiles str按字符拆分，并替换为词汇表中的序号\n",
    "class Smiles_tokenizer():\n",
    "    def __init__(self, pad_token, regex, vocab_file, max_length):\n",
    "        self.pad_token = pad_token\n",
    "        self.regex = regex\n",
    "        self.vocab_file = vocab_file\n",
    "        self.max_length = max_length\n",
    "\n",
    "        with open(self.vocab_file, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "        lines = [line.strip(\"\\n\") for line in lines]\n",
    "        vocab_dic = {}\n",
    "        for index, token in enumerate(lines):\n",
    "            vocab_dic[token] = index\n",
    "        self.vocab_dic = vocab_dic\n",
    "\n",
    "    def _regex_match(self, smiles):\n",
    "        regex_string = r\"(\" + self.regex + r\"|\"\n",
    "        regex_string += r\".)\"\n",
    "        prog = re.compile(regex_string)\n",
    "\n",
    "        tokenised = []\n",
    "        for smi in smiles:\n",
    "            tokens = prog.findall(smi)\n",
    "            if len(tokens) > self.max_length:\n",
    "                tokens = tokens[:self.max_length]\n",
    "            tokenised.append(tokens) # 返回一个所有的字符串列表\n",
    "        return tokenised\n",
    "\n",
    "    def tokenize(self, smiles):\n",
    "        tokens = self._regex_match(smiles)\n",
    "        # 添加上表示开始和结束的token：<cls>, <end>\n",
    "        tokens = [[\"<CLS>\"] + token + [\"<SEP>\"] for token in tokens]\n",
    "        tokens = self._pad_seqs(tokens, self.pad_token)\n",
    "        token_idx = self._pad_token_to_idx(tokens)\n",
    "        return tokens, token_idx\n",
    "\n",
    "    def _pad_seqs(self, seqs, pad_token):\n",
    "        pad_length = max([len(seq) for seq in seqs])\n",
    "        padded = [seq + ([pad_token] * (pad_length - len(seq))) for seq in seqs]\n",
    "        return padded\n",
    "\n",
    "    def _pad_token_to_idx(self, tokens):\n",
    "        idx_list = []\n",
    "        for token in tokens:\n",
    "            tokens_idx = []\n",
    "            for i in token:\n",
    "                if i in self.vocab_dic.keys():\n",
    "                    tokens_idx.append(self.vocab_dic[i])\n",
    "                else:\n",
    "                    self.vocab_dic[i] = max(self.vocab_dic.values()) + 1\n",
    "                    tokens_idx.append(self.vocab_dic[i])\n",
    "            idx_list.append(tokens_idx)\n",
    "\n",
    "        return idx_list\n",
    "\n",
    "# 读数据并处理\n",
    "def read_data(file_path, train=True):\n",
    "    df = pd.read_csv(file_path)\n",
    "    reactant1 = df[\"Reactant1\"].tolist()\n",
    "    reactant2 = df[\"Reactant2\"].tolist()\n",
    "    product = df[\"Product\"].tolist()\n",
    "    additive = df[\"Additive\"].tolist()\n",
    "    solvent = df[\"Solvent\"].tolist()\n",
    "    if train:\n",
    "        react_yield = df[\"Yield\"].tolist()\n",
    "    else:\n",
    "        react_yield = [0 for i in range(len(reactant1))]\n",
    "\n",
    "    # 将reactant拼到一起，之间用.分开。product也拼到一起，用>分开\n",
    "    input_data_list = []\n",
    "    for react1, react2, prod, addi, sol in zip(reactant1, reactant2, product, additive, solvent):\n",
    "        input_info = \".\".join([react1, react2])\n",
    "        input_info = \">\".join([input_info, prod])\n",
    "        input_data_list.append(input_info)\n",
    "    output = [(react, y) for react, y in zip(input_data_list, react_yield)]\n",
    "\n",
    "    # 下面的代码将reactant\\additive\\solvent拼到一起，之间用.分开。product也拼到一起，用>分开\n",
    "    '''\n",
    "    input_data_list = []\n",
    "    for react1, react2, prod, addi, sol in zip(reactant1, reactant2, product, additive, solvent):\n",
    "        input_info = \".\".join([react1, react2, addi, sol])\n",
    "        input_info = \">\".join([input_info, prod])\n",
    "        input_data_list.append(input_info)\n",
    "    output = [(react, y) for react, y in zip(input_data_list, react_yield)]\n",
    "    '''\n",
    "\n",
    "    # # 统计seq length，序列的长度是一个重要的参考，可以使用下面的代码统计查看以下序列长度的分布\n",
    "    # seq_length = [len(i[0]) for i in output]\n",
    "    # seq_length_400 = [len(i[0]) for i in output if len(i[0])>200]\n",
    "    # print(len(seq_length_400) / len(seq_length))\n",
    "    # seq_length.sort(reverse=True)\n",
    "    # plt.plot(range(len(seq_length)), seq_length)\n",
    "    # plt.title(\"templates frequence\")\n",
    "    # plt.show()\n",
    "    return output\n",
    "\n",
    "class ReactionDataset(Dataset):\n",
    "    def __init__(self, data: List[Tuple[List[str], float]]):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "def collate_fn(batch):\n",
    "    REGEX = r\"\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]\"\n",
    "    tokenizer = Smiles_tokenizer(\"<PAD>\", REGEX, \"/home/ma-user/work/AI-chemistry/mp/vocab_full.txt\", max_length=300)\n",
    "    smi_list = []\n",
    "    yield_list = []\n",
    "    for i in batch:\n",
    "        smi_list.append(i[0])\n",
    "        yield_list.append(i[1])\n",
    "    tokenizer_batch = torch.tensor(tokenizer.tokenize(smi_list)[1])\n",
    "    yield_list = torch.tensor(yield_list)\n",
    "    return tokenizer_batch, yield_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-u5hYctuYxrh",
    "outputId": "2ebb2756-4433-48dc-86ae-13f8b613d1ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W compiler_depend.ts:623] Warning: expandable_segments currently defaults to false. You can enable this feature by `export PYTORCH_NPU_ALLOC_CONF = expandable_segments:True`. (function operator())\n",
      "/usr/local/Ascend/ascend-toolkit/8.0.RC2/python/site-packages/tbe/tvm/contrib/ccec.py:854: DeprecationWarning: invalid escape sequence \\L\n",
      "  if not dirpath.find(\"AppData\\Local\\Temp\"):\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/classifier/transdata/transdata_classifier.py:222: DeprecationWarning: invalid escape sequence \\B\n",
      "  \"\"\"\n",
      "/usr/local/Ascend/ascend-toolkit/latest/python/site-packages/tbe/dsl/unify_schedule/vector/transdata/common/graph/transdata_graph_info.py:143: DeprecationWarning: invalid escape sequence \\c\n",
      "  \"\"\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Train Loss: 0.270\n",
      "Total running time: 7.45 minutes\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    ## super param\n",
    "    N = 10  #int / int(len(dataset) * 1)  # 或者你可以设置为数据集大小的一定比例，如 int(len(dataset) * 0.1)\n",
    "    NUM_EMBED = 294 # nn.Embedding()\n",
    "    INPUT_SIZE = 300 # src length\n",
    "    HIDDEN_SIZE = 512\n",
    "    OUTPUT_SIZE = 512\n",
    "    NUM_LAYERS = 10\n",
    "    DROPOUT = 0.2\n",
    "    CLIP = 1 # CLIP value\n",
    "    N_EPOCHS = 1\n",
    "    LR = 0.001\n",
    "\n",
    "    start_time = time.time()  # 开始计时\n",
    "    # device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    # device = 'cpu'\n",
    "    data = read_data(\"/home/ma-user/work/AI-chemistry/mp/dataset/train_data.csv\")\n",
    "    dataset = ReactionDataset(data)\n",
    "    subset_indices = list(range(N))\n",
    "    subset_dataset = Subset(dataset, subset_indices)\n",
    "    train_loader = DataLoader(dataset, batch_size=128, shuffle=True, collate_fn=collate_fn)\n",
    "    device=torch.device('npu:0')\n",
    "    model = RNNModel(NUM_EMBED, INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS, DROPOUT, device).to(device)\n",
    "    model.train()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "    # criterion = nn.MSELoss() # MSE\n",
    "    criterion = nn.L1Loss() # MAE\n",
    "\n",
    "    best_loss = 10\n",
    "    for epoch in range(N_EPOCHS):\n",
    "        epoch_loss = 0\n",
    "        for i, (src, y) in enumerate(train_loader):\n",
    "            src, y = src.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(src)\n",
    "            loss = criterion(output, y)\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "            loss_in_a_epoch = epoch_loss / len(train_loader)\n",
    "        print(f'Epoch: {epoch+1:02} | Train Loss: {loss_in_a_epoch:.3f}')\n",
    "        if loss_in_a_epoch < best_loss:\n",
    "            # 在训练循环结束后保存模型\n",
    "            torch.save(model.state_dict(), '/home/ma-user/work/AI-chemistry/mp/model/RNN.pth')\n",
    "    end_time = time.time()  # 结束计时\n",
    "    # 计算并打印运行时间\n",
    "    elapsed_time_minute = (end_time - start_time)/60\n",
    "    print(f\"Total running time: {elapsed_time_minute:.2f} minutes\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NZNumizWYxrh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!!!\n"
     ]
    }
   ],
   "source": [
    "# 生成结果文件\n",
    "def predicit_and_make_submit_file(model_file, output_file):\n",
    "    NUM_EMBED = 294\n",
    "    INPUT_SIZE = 300\n",
    "    HIDDEN_SIZE = 512\n",
    "    OUTPUT_SIZE = 512\n",
    "    NUM_LAYERS = 10\n",
    "    DROPOUT = 0.2\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    test_data = read_data(\"/home/ma-user/work/AI-chemistry/mp/dataset/round1_test_data.csv\", train=False)\n",
    "    test_dataset = ReactionDataset(test_data)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "    model = RNNModel(NUM_EMBED, INPUT_SIZE, HIDDEN_SIZE, OUTPUT_SIZE, NUM_LAYERS, DROPOUT, device).to(device)\n",
    "    # 加载最佳模型\n",
    "    model.load_state_dict(torch.load(model_file))\n",
    "    model.eval()\n",
    "    output_list = []\n",
    "    for i, (src, y) in enumerate(test_loader):\n",
    "        src, y = src.to(device), y.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(src)\n",
    "            output_list += output.detach().tolist()\n",
    "    ans_str_lst = ['rxnid,Yield']\n",
    "    for idx,y in enumerate(output_list):\n",
    "        ans_str_lst.append(f'test{idx+1},{y:.4f}')\n",
    "    with open(output_file,'w') as fw:\n",
    "        fw.writelines('\\n'.join(ans_str_lst))\n",
    "\n",
    "    print(\"done!!!\")\n",
    "\n",
    "\n",
    "\n",
    "predicit_and_make_submit_file(\"/home/ma-user/work/AI-chemistry/mp/model/RNN.pth\",\"/home/ma-user/work/AI-chemistry/mp/output/RNN_submit.txt\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PyTorch-2.1.0",
   "language": "python",
   "name": "pytorch-2.1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
