{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFkzxuXl41QO"
      },
      "source": [
        "# 安装环境"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G_paPcK-41QP",
        "outputId": "2ec4fa73-cffe-4c4e-835b-1827d7826890"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: scikit-optimize in /usr/local/lib/python3.10/dist-packages (0.10.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.4.2)\n",
            "Requirement already satisfied: pyaml>=16.9 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.9.0)\n",
            "Requirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (1.5.2)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from scikit-optimize) (24.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from pyaml>=16.9->scikit-optimize) (6.0.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=1.0.0->scikit-optimize) (3.5.0)\n",
            "fatal: destination path 'AI-chemistry' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "# !pip install torchtext\n",
        "!pip install pandas\n",
        "!pip install scikit-optimize\n",
        "!git clone https://github.com/xzxg001/AI-chemistry.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spoEXJ9141QQ"
      },
      "source": [
        "# 对数据处理进行初步尝试"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "24CkVmGX41QQ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from typing import List, Tuple\n",
        "import re\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import time\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4jNsCZYC41QR"
      },
      "outputs": [],
      "source": [
        "# tokenizer，鉴于SMILES的特性，这里需要自己定义tokenizer和vocab\n",
        "# 这里直接将smiles str按字符拆分，并替换为词汇表中的序号\n",
        "class Smiles_tokenizer():\n",
        "    def __init__(self, pad_token, regex, vocab_file, max_length):\n",
        "        self.pad_token = pad_token\n",
        "        self.regex = regex\n",
        "        self.vocab_file = vocab_file\n",
        "        self.max_length = max_length\n",
        "\n",
        "        with open(self.vocab_file, \"r\") as f:\n",
        "            lines = f.readlines()\n",
        "        lines = [line.strip(\"\\n\") for line in lines]\n",
        "        vocab_dic = {}\n",
        "        for index, token in enumerate(lines):\n",
        "            vocab_dic[token] = index\n",
        "        self.vocab_dic = vocab_dic\n",
        "\n",
        "    def _regex_match(self, smiles):\n",
        "        regex_string = r\"(\" + self.regex + r\"|\"\n",
        "        regex_string += r\".)\"\n",
        "        prog = re.compile(regex_string)\n",
        "\n",
        "        tokenised = []\n",
        "        for smi in smiles:\n",
        "            tokens = prog.findall(smi)\n",
        "            if len(tokens) > self.max_length:\n",
        "                tokens = tokens[:self.max_length]\n",
        "            tokenised.append(tokens) # 返回一个所有的字符串列表\n",
        "        return tokenised\n",
        "\n",
        "    def tokenize(self, smiles):\n",
        "        tokens = self._regex_match(smiles)\n",
        "        # 添加上表示开始和结束的token：<cls>, <end>\n",
        "        tokens = [[\"<CLS>\"] + token + [\"<SEP>\"] for token in tokens]\n",
        "        tokens = self._pad_seqs(tokens, self.pad_token)\n",
        "        token_idx = self._pad_token_to_idx(tokens)\n",
        "        return tokens, token_idx\n",
        "\n",
        "    def _pad_seqs(self, seqs, pad_token):\n",
        "        pad_length = max([len(seq) for seq in seqs])\n",
        "        padded = [seq + ([pad_token] * (pad_length - len(seq))) for seq in seqs]\n",
        "        return padded\n",
        "\n",
        "    def _pad_token_to_idx(self, tokens):\n",
        "        idx_list = []\n",
        "        new_vocab = []\n",
        "        for token in tokens:\n",
        "            tokens_idx = []\n",
        "            for i in token:\n",
        "                if i in self.vocab_dic.keys():\n",
        "                    tokens_idx.append(self.vocab_dic[i])\n",
        "                else:\n",
        "                    new_vocab.append(i)\n",
        "                    self.vocab_dic[i] = max(self.vocab_dic.values()) + 1\n",
        "                    tokens_idx.append(self.vocab_dic[i])\n",
        "            idx_list.append(tokens_idx)\n",
        "\n",
        "        with open(\"../new_vocab_list.txt\", \"a\") as f:\n",
        "            for i in new_vocab:\n",
        "                f.write(i)\n",
        "                f.write(\"\\n\")\n",
        "\n",
        "        return idx_list\n",
        "\n",
        "    def _save_vocab(self, vocab_path):\n",
        "        with open(vocab_path, \"w\") as f:\n",
        "            for i in self.vocab_dic.keys():\n",
        "                f.write(i)\n",
        "                f.write(\"\\n\")\n",
        "        print(\"update new vocab!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "rHPpF6GQ41QR"
      },
      "outputs": [],
      "source": [
        "# 处理数据\n",
        "\n",
        "def read_data(file_path, train=True):\n",
        "    df = pd.read_csv(file_path)\n",
        "    reactant1 = df[\"Reactant1\"].tolist()\n",
        "    reactant2 = df[\"Reactant2\"].tolist()\n",
        "    product = df[\"Product\"].tolist()\n",
        "    additive = df[\"Additive\"].tolist()\n",
        "    solvent = df[\"Solvent\"].tolist()\n",
        "    if train:\n",
        "        react_yield = df[\"Yield\"].tolist()\n",
        "    else:\n",
        "        react_yield = [0 for i in range(len(reactant1))]\n",
        "\n",
        "    # 将reactant\\additive\\solvent拼到一起，之间用.分开。product也拼到一起，用>>分开\n",
        "    input_data_list = []\n",
        "    for react1, react2, prod, addi, sol in zip(reactant1, reactant2, product, additive, solvent):\n",
        "        # input_info = \".\".join([react1, react2, addi, sol])\n",
        "        input_info = \".\".join([react1, react2])\n",
        "        input_info = \">\".join([input_info, prod])\n",
        "        input_data_list.append(input_info)\n",
        "    output = [(react, y) for react, y in zip(input_data_list, react_yield)]\n",
        "\n",
        "    return output\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "omfbLOUW41QR"
      },
      "outputs": [],
      "source": [
        "# 定义数据集\n",
        "class ReactionDataset(Dataset):\n",
        "    def __init__(self, data: List[Tuple[List[str], float]]):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "def collate_fn(batch):\n",
        "    REGEX = r\"\\[[^\\]]+]|Br?|Cl?|N|O|S|P|F|I|b|c|n|o|s|p|\\(|\\)|\\.|=|#|-|\\+|\\\\\\\\|\\/|:|~|@|\\?|>|\\*|\\$|\\%[0-9]{2}|[0-9]\"\n",
        "    tokenizer = Smiles_tokenizer(\"<PAD>\", REGEX, \"/content/AI-chemistry/mp/vocab_full.txt\", 300)\n",
        "    smi_list = []\n",
        "    yield_list = []\n",
        "    for i in batch:\n",
        "        smi_list.append(i[0])\n",
        "        yield_list.append(i[1])\n",
        "    tokenizer_batch = torch.tensor(tokenizer.tokenize(smi_list)[1])\n",
        "    yield_list = torch.tensor(yield_list)\n",
        "    return tokenizer_batch, yield_list\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "faUZr4-F41QS"
      },
      "outputs": [],
      "source": [
        "# 模型\n",
        "'''\n",
        "直接采用一个transformer encoder model就好了\n",
        "'''\n",
        "class TransformerEncoderModel(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, num_heads, fnn_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, d_model)\n",
        "        self.layerNorm = nn.LayerNorm(d_model)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model,\n",
        "                                                        nhead=num_heads,\n",
        "                                                        dim_feedforward=fnn_dim,\n",
        "                                                        dropout=dropout,\n",
        "                                                        batch_first=True,\n",
        "                                                        norm_first=True # pre-layernorm\n",
        "                                                        )\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer,\n",
        "                                                         num_layers=num_layers,\n",
        "                                                         norm=self.layerNorm)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lc = nn.Sequential(nn.Linear(d_model, 256),\n",
        "                                nn.Sigmoid(),\n",
        "                                nn.Linear(256, 96),\n",
        "                                nn.Sigmoid(),\n",
        "                                nn.Linear(96, 1))\n",
        "\n",
        "    def forward(self, src):\n",
        "        # src shape: [batch_size, src_len]\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        # embedded shape: [batch_size, src_len, d_model]\n",
        "        outputs = self.transformer_encoder(embedded)\n",
        "        # outputs shape: [batch_size, src_len, d_model]\n",
        "\n",
        "        # fisrt\n",
        "        z = outputs[:,0,:]\n",
        "        # z = torch.sum(outputs, dim=1)\n",
        "        # print(z)\n",
        "        # z shape: [bs, d_model]\n",
        "        outputs = self.lc(z)\n",
        "        # print(outputs)\n",
        "        # outputs shape: [bs, 1]\n",
        "        return outputs.squeeze(-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "XDv4kJyW41QS"
      },
      "outputs": [],
      "source": [
        "def adjust_learning_rate(optimizer, epoch, start_lr):\n",
        "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
        "    lr = start_lr * (0.1 ** (epoch // 3))\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L8hk_Uv341QS"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from skopt import gp_minimize\n",
        "from skopt.space import Real, Integer\n",
        "from skopt.utils import use_named_args\n",
        "\n",
        "# 假设 read_data, ReactionDataset, collate_fn, TransformerEncoderModel 等函数和类已经定义\n",
        "\n",
        "def prepare_data(data_path, batch_size, test_size=0.1, valid_size=0.1, scaler=None):\n",
        "    data = read_data(data_path)\n",
        "    dataset = ReactionDataset(data)\n",
        "\n",
        "    # 应用归一化\n",
        "    if scaler:\n",
        "        dataset.data = scaler.fit_transform(dataset.data)\n",
        "\n",
        "    test_size = int(len(dataset) * test_size)\n",
        "    valid_size = int(len(dataset) * valid_size)\n",
        "    train_size = len(dataset) - test_size - valid_size\n",
        "\n",
        "    train_dataset, valid_test_dataset = random_split(dataset, [train_size, len(dataset) - train_size])\n",
        "    valid_dataset, test_dataset = random_split(valid_test_dataset, [valid_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
        "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n",
        "\n",
        "    return train_loader, valid_loader, test_loader\n",
        "\n",
        "class TransformerEncoderModel(nn.Module):\n",
        "    def __init__(self, input_dim, d_model, num_heads, fnn_dim, num_layers, dropout):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, d_model)\n",
        "        self.layerNorm = nn.LayerNorm(d_model)\n",
        "        self.encoder_layer = nn.TransformerEncoderLayer(d_model=d_model,\n",
        "                                                        nhead=num_heads,\n",
        "                                                        dim_feedforward=fnn_dim,\n",
        "                                                        dropout=dropout,\n",
        "                                                        batch_first=True,\n",
        "                                                        norm_first=True)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layer, num_layers=num_layers, norm=self.layerNorm)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.lc = nn.Sequential(nn.Linear(d_model, 256),\n",
        "                                nn.Sigmoid(),\n",
        "                                nn.Linear(256, 96),\n",
        "                                nn.Sigmoid(),\n",
        "                                nn.Linear(96, 1))\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        outputs = self.transformer_encoder(embedded)\n",
        "        z = outputs[:,0,:]\n",
        "        outputs = self.lc(z)\n",
        "        return outputs.squeeze(-1)\n",
        "\n",
        "def train(**params):\n",
        "    INPUT_DIM = 292\n",
        "    D_MODEL = int(params['D_MODEL'])\n",
        "    NUM_HEADS = int(params['NUM_HEADS'])\n",
        "    FNN_DIM = int(params['FNN_DIM'])\n",
        "    NUM_LAYERS = int(params['NUM_LAYERS'])\n",
        "    DROPOUT = params['DROPOUT']\n",
        "    LR = params['LR']\n",
        "    CLIP = 1\n",
        "    N_EPOCHS = 40\n",
        "    model_file = \"/content/AI-chemistry/mp/model/transformer.pth\"\n",
        "\n",
        "    train_loader, valid_loader, _ = prepare_data(\n",
        "        \"/content/AI-chemistry/mp/dataset/round1_train_data.csv\",\n",
        "        batch_size=128,\n",
        "        scaler=StandardScaler()\n",
        "    )\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = TransformerEncoderModel(INPUT_DIM, D_MODEL, NUM_HEADS, FNN_DIM, NUM_LAYERS, DROPOUT).to(device)\n",
        "    model.train()\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=0.01)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    best_valid_loss = float('inf')\n",
        "    for epoch in range(N_EPOCHS):\n",
        "        epoch_loss = 0\n",
        "        for i, (src, y) in enumerate(train_loader):\n",
        "            src, y = src.to(device), y.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src)\n",
        "            loss = criterion(output, y)\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP)\n",
        "            optimizer.step()\n",
        "            epoch_loss += loss.detach().item()\n",
        "\n",
        "            if i % 50 == 0:\n",
        "                print(f'Step: {i} | Train Loss: {epoch_loss:.4f}')\n",
        "\n",
        "        scheduler.step(epoch_loss / len(train_loader))\n",
        "        print(f'Epoch: {epoch+1:02} | Train Loss: {epoch_loss / len(train_loader):.3f}')\n",
        "\n",
        "        valid_loss = 0\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            for src, y in valid_loader:\n",
        "                src, y = src.to(device), y.to(device)\n",
        "                output = model(src)\n",
        "                valid_loss += criterion(output, y).item()\n",
        "\n",
        "        valid_loss /= len(valid_loader)\n",
        "        print(f'Epoch: {epoch+1:02} | Valid Loss: {valid_loss:.3f}')\n",
        "        model.train()\n",
        "\n",
        "        if valid_loss < best_valid_loss:\n",
        "            best_valid_loss = valid_loss\n",
        "            torch.save(model.state_dict(), model_file)\n",
        "\n",
        "    return {'loss': best_valid_loss, 'status': 'ok'}\n",
        "\n",
        "# 定义超参数搜索空间\n",
        "search_spaces = {\n",
        "    'D_MODEL': Integer(128, 512),\n",
        "    'NUM_HEADS': Integer(2, 8),\n",
        "    'FNN_DIM': Integer(256, 2048),\n",
        "    'NUM_LAYERS': Integer(2, 6),\n",
        "    'DROPOUT': Real(0.1, 0.5),\n",
        "    'LR': Real(1e-5, 1e-1, prior='log-uniform'),\n",
        "}\n",
        "\n",
        "# 使用贝叶斯优化\n",
        "@use_named_args(search_spaces)\n",
        "def objective(D_MODEL, NUM_HEADS, FNN_DIM, NUM_LAYERS, DROPOUT, LR):\n",
        "    return train(D_MODEL=D_MODEL, NUM_HEADS=NUM_HEADS, FNN_DIM=FNN_DIM, NUM_LAYERS=NUM_LAYERS, DROPOUT=DROPOUT, LR=LR)\n",
        "\n",
        "res = gp_minimize(objective, search_spaces, n_calls=50, random_state=0)\n",
        "\n",
        "print(\"最佳超参数: \", res.x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpQZSJUh41QS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6acf0520-22b4-4463-a846-353d8d478dd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
            "  warnings.warn(\n",
            "<ipython-input-31-9871bf896699>:46: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(model_file, map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded from /content/AI-chemistry/mp/model/transformer.pth\n",
            "Test MSE Loss: 0.048\n",
            "Test MAE Loss: 0.180\n",
            "Test R-squared: 0.168\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "def test(model, test_loader, device):\n",
        "    model.eval()\n",
        "    total_mse_loss = 0\n",
        "    total_mae_loss = 0\n",
        "    total_r2 = 0\n",
        "    total_count = 0\n",
        "    all_predictions = []\n",
        "    all_targets = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for src, y in test_loader:\n",
        "            src, y = src.to(device), y.to(device)\n",
        "            output = model(src)\n",
        "            mse_loss = nn.MSELoss()(output, y)\n",
        "            mae_loss = nn.L1Loss()(output, y)\n",
        "            r2 = r2_score(y.cpu().numpy(), output.cpu().numpy())  # 计算 R-squared\n",
        "            total_mse_loss += mse_loss.item() * len(y)\n",
        "            total_mae_loss += mae_loss.item() * len(y)\n",
        "            total_r2 += r2 * len(y)\n",
        "            total_count += len(y)\n",
        "\n",
        "            # 保存所有预测和目标值用于整体 R-squared 计算\n",
        "            all_predictions.extend(output.cpu().numpy().flatten())\n",
        "            all_targets.extend(y.cpu().numpy().flatten())\n",
        "\n",
        "    avg_mse_loss = total_mse_loss / total_count if total_count > 0 else 0\n",
        "    avg_mae_loss = total_mae_loss / total_count if total_count > 0 else 0\n",
        "    avg_r2 = total_r2 / total_count if total_count > 0 else 0\n",
        "\n",
        "    print(f'Test MSE Loss: {avg_mse_loss:.3f}')\n",
        "    print(f'Test MAE Loss: {avg_mae_loss:.3f}')\n",
        "    print(f'Test R-squared: {avg_r2:.3f}')\n",
        "\n",
        "    return avg_mse_loss, avg_mae_loss, avg_r2\n",
        "\n",
        "# 加载模型并测试\n",
        "def load_and_test(model_file, test_loader, INPUT_DIM, D_MODEL, NUM_HEADS, FNN_DIM, NUM_LAYERS, DROPOUT):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = TransformerEncoderModel(INPUT_DIM, D_MODEL, NUM_HEADS, FNN_DIM, NUM_LAYERS, DROPOUT).to(device)\n",
        "    if os.path.exists(model_file):\n",
        "        model.load_state_dict(torch.load(model_file, map_location=device))\n",
        "        model.eval()\n",
        "        print(f\"Model loaded from {model_file}\")\n",
        "        return test(model, test_loader, device)\n",
        "    else:\n",
        "        print(\"No existing model found. Please train the model first.\")\n",
        "        return None\n",
        "\n",
        "# 调用测试函数\n",
        "# 确保在调用之前已经定义了 model_file, test_loader, INPUT_DIM, D_MODEL, NUM_HEADS, FNN_DIM, NUM_LAYERS, DROPOUT\n",
        "mse_loss, mae_loss, r2_score = load_and_test(model_file, test_loader, INPUT_DIM, D_MODEL, NUM_HEADS, FNN_DIM, NUM_LAYERS, DROPOUT)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}